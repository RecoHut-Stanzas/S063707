{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Robust Recommender from Noisy Implicit Feedback"
      ],
      "metadata": {
        "id": "lIYdn1woOS1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executive summary\n",
        "\n",
        "| | |\n",
        "| --- | --- |\n",
        "| Problem | It is of importance to account for the inevitable noises in implicit feedback. However, little work on recommendation has taken the noisy nature of implicit feedback into consideration. |\n",
        "| Prblm Stmt. | We formulate a denoising recommender training task as $Θ^∗ = \\text{argmin}_Θ \\mathcal{L}(\\textit{denoise}(\\bar{D}))$ aiming to learn a reliable recommender model with parameters $Θ^∗$ by denoising implicit feedback $\\bar{D}$. Formally, by assuming the existence of inconsistency between $y_{ui}^∗$ and $\\bar{y}_{ui}$, we define noisy interactions (a.k.a. false-positive interactions) as $\\{(u, i)|y_{ui}^∗ = 0 ∧ \\bar{y}_{ui} = 1\\}$. |\n",
        "| Solution | Adaptive Denoising Training (ADT), which adaptively prunes the noisy interactions by two paradigms - Truncated Loss and Reweighted Loss. Furthermore, we consider extra feedback (e.g., rating) as auxiliary signal and employ three strategies to incorporate extra feedback into ADT: fine-tuning, warm-up training, and colliding inference. |\n",
        "| Dataset | Adressa, Amazon-books, Yelp2018. |\n",
        "| Preprocessing | We split the dataset into training, validation, and testing sets, and explored two experimental settings: 1) Extra feedback is unavailable during training. To evaluate the performance of denoising implicit feedback, we kept all interactions, including the false-positive ones, in training and validation, and tested the models only on true-positive interactions. 2) Sparse extra feedback is available during training. We assume that partial true-positive interactions have already been known, which will be used to verify the performance of the proposed three strategies: fine-tuning, warm-up training, and colliding inference. |\n",
        "| Metrics | Recall, NDCG |\n",
        "| Hyperparams | For GMF and NeuMF, the factor numbers of users and items are both 32. As to CDAE, the hidden size of MLP is set as 200. In addition, Adam is applied to optimize all the parameters with the learning rate initialized as 0.001 and he batch size set as 1,024. As to the ADT strategies, they have three hyper-parameters in total: α and max in the T-CE loss, and β in the R-CE loss. In detail, max is searched in {0.05, 0.1, 0.2} and β is tuned in {0.05, 0.1, ..., 0.25, 0.5, 1.0}. As for α, we controlled its range by adjusting the iteration number N to the maximum drop rate max, and N is adjusted in {1k, 5k, 10k, 20k, 30k}. In colliding inference, the number of neighbors Nu is tuned in {1, 3, 5, 10, 20, 50, 100}, wj is set as 1/|Nu|, and λ is searched in {0, 0.1, 0.2, ..., 1}. We used the validation set to tune the hyper-parameters and reported the performance on the testing set. |\n",
        "| Models | GMF, NMF, CDAE, {GMF, NMF, CDAE}+T_CE, {GMF, NMF, CDAE}+R_CE |\n",
        "| Cluster | Python 3.6+, PyTorch |\n",
        "| Tags | `LossReweighting`, `TruncatedLoss`, `MatrixFactorization`, `Denoising` |\n",
        "| Credits | Wenjie Wang |"
      ],
      "metadata": {
        "id": "wVfZ0xpJb4nI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "\n",
        "![The comparison between normal training (a); two prior solutions (b) and (c); and the proposed denoising training without additional data (d) and with extra feedback (e). Note that red lines in the user-item graph denote false-positive interactions, and extra feedback usually cannot identify all false-positive ones due to the sparsity issue.](https://github.com/RecoHut-Stanzas/S063707/raw/main/images/img1.png)\n",
        "\n",
        "The comparison between normal training (a); two prior solutions (b) and (c); and the proposed denoising training without additional data (d) and with extra feedback (e). Note that red lines in the user-item graph denote false-positive interactions, and extra feedback usually cannot identify all false-positive ones due to the sparsity issue.\n",
        "\n",
        "## Model\n",
        "\n",
        "ADT either discards or reweighs the interactions with large loss values to reduce their influences on the recommender training. Towards this end, we devise two paradigms to formulate loss functions for denoising training without using extra feedback:\n",
        "\n",
        "- Truncated Loss. This is to truncate the loss values of hard interactions to 0 with a dynamic threshold function.\n",
        "- Reweighted Loss. It adaptively assigns hard interactions with smaller weights during training.\n",
        "\n",
        "These two paradigms can be applied to various recommendation loss functions, e.g., CE loss, square loss, and BPR loss. We take CE loss as an example to elaborate them.\n",
        "\n",
        "### Truncated Cross-Entropy Loss\n",
        "\n",
        "Functionally speaking, the Truncated Cross-Entropy (shorted as T-CE) loss discards positive interactions according to the values of CE loss. Formally, we define it as:\n",
        "\n",
        "$$\\mathcal{L}_{T-CE}(u,i) = \\begin{cases} 0, & \\mathcal{L}_{CE}(u,i)>\\tau \\wedge\\bar{y}_{ui}=1 \\\\ \\mathcal{L}_{CE}(u,i), & \\text{otherwise}, \\end{cases}$$\n",
        "\n",
        "### Re-weighted Cross-Entropy Loss\n",
        "\n",
        "Generally, the Re-weighted Cross-Entropy (shorted as R-CE) loss down-weights the hard positive interactions. Formally,\n",
        "\n",
        "$$\\mathcal{L}_{R-CE}(u,i) = \\omega(u,i)\\mathcal{L}_{CE}(u,i)$$\n",
        "\n",
        "where $\\omega(u, i)$ is a weight function that adjusts the contribution of an interaction to the training objective.\n",
        "\n",
        "### ADT with Extra Feedback\n",
        "\n",
        "Although extra feedback (e.g., ratings) is usually sparse, it is reliable to reflect the actual user satisfaction, i.e., indicating the true-positive interactions. We thus further utilize extra feedback for ADT when available. By considering the order of training with implicit feedback and extra feedback, we introduce two training strategies: fine-tuning and warm-up training.\n",
        "\n",
        "![Illustration of fine-tuning and warm-up training with extra feedback.](https://github.com/RecoHut-Stanzas/S063707/raw/main/images/img2.png)\n",
        "\n",
        "Illustration of fine-tuning and warm-up training with extra feedback.\n",
        "\n",
        "## Tutorials\n",
        "\n",
        "### Training GMF with Truncated and Reweighted Denoising Losses on Yelp Dataset\n",
        "\n",
        "[direct link to notebook →](https://github.com/RecoHut-Stanzas/S063707/blob/main/nbs/P254192_Training_GMF_with_Truncated_and_Reweighted_Denoising_Losses_on_Yelp_Dataset.ipynb)\n",
        "\n",
        "![https://github.com/RecoHut-Stanzas/S063707/raw/main/images/process_flow.svg](https://github.com/RecoHut-Stanzas/S063707/raw/main/images/process_flow.svg)\n",
        "\n",
        "## References\n",
        "\n",
        "1. [https://github.com/RecoHut-Stanzas/S063707](https://github.com/RecoHut-Stanzas/S063707)\n",
        "2. [https://arxiv.org/pdf/2112.01160v1.pdf](https://arxiv.org/pdf/2112.01160v1.pdf)\n",
        "3. [https://github.com/WenjieWWJ/DenoisingRec](https://github.com/WenjieWWJ/DenoisingRec)"
      ],
      "metadata": {
        "id": "2t0XD6_j12FW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}